{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#0.import and random seeds\n"
      ],
      "metadata": {
        "id": "O6G1rRnYt22w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WhlcLeoVsyWt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributions as td\n",
        "import torch.utils.data\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "import os\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "#dataset\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)                           # Python built-in random module\n",
        "    np.random.seed(seed)                        # NumPy random generator\n",
        "    torch.manual_seed(seed)                     # PyTorch CPU random seed\n",
        "    torch.cuda.manual_seed(seed)                # PyTorch current GPU random seed\n",
        "    torch.cuda.manual_seed_all(seed)            # PyTorch all GPUs random seed\n",
        "    torch.backends.cudnn.deterministic = True   # Ensure deterministic behavior in cuDNN\n",
        "    torch.backends.cudnn.benchmark = False      # Disable auto-optimization to prevent non-deterministic behavior\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)    # Control hash-based randomness in Python\n",
        "\n",
        "set_seed(42)\n"
      ],
      "metadata": {
        "id": "OtQ3OkKP1FyY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.Dataset  \n",
        "just copy the code provided by the professor\n",
        "\n"
      ],
      "metadata": {
        "id": "4OuS5xx-nLi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def subsample(data, targets, num_data, num_classes):\n",
        "    idx = targets < num_classes  # Select samples with class labels less than num_classes (e.g., only classes 0, 1, 2)\n",
        "    new_data = data[idx][:num_data].unsqueeze(1).to(torch.float32) / 255  # Select the first num_data images and normalize to [0,1]\n",
        "    new_targets = targets[idx][:num_data]  # Select corresponding labels for the subsampled images\n",
        "    return torch.utils.data.TensorDataset(new_data, new_targets)  # Create a TensorDataset with the filtered images and labels\n",
        "\n",
        "num_train_data = 2048\n",
        "num_classes = 3\n",
        "\n",
        "train_tensors = datasets.MNIST(\n",
        "    \"data/\", train=True, download=True,\n",
        "    transform=transforms.Compose([transforms.ToTensor()])  # Convert images to tensors\n",
        ")\n",
        "\n",
        "test_tensors = datasets.MNIST(\n",
        "    \"data/\", train=False, download=True,\n",
        "    transform=transforms.Compose([transforms.ToTensor()])  # Convert images to tensors\n",
        ")\n",
        "train_data = subsample(\n",
        "    train_tensors.data, train_tensors.targets,\n",
        "    num_train_data, num_classes\n",
        ")\n",
        "test_data = subsample(\n",
        "    test_tensors.data, test_tensors.targets,\n",
        "    num_train_data, num_classes\n",
        ")\n",
        "\n",
        "batch_size=32\n",
        "mnist_train_loader = torch.utils.data.DataLoader(\n",
        "    train_data, batch_size=batch_size, shuffle=True\n",
        ")\n",
        "mnist_test_loader = torch.utils.data.DataLoader(\n",
        "    test_data, batch_size=batch_size, shuffle=False\n",
        ")\n",
        "latent_dim=2\n",
        "M=latent_dim\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "1ul7SutXjtmn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "672265d2-3599-44d4-8b8b-e01567ad4721"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:11<00:00, 899kB/s] \n",
            "100%|██████████| 28.9k/28.9k [00:01<00:00, 26.8kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.27MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.13MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.GaussianPrior and Encoder/Decoder\n",
        "Just copy the code provided by the professor\n"
      ],
      "metadata": {
        "id": "RKGEA5qGnR4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianPrior(nn.Module):\n",
        "    def __init__(self, M):\n",
        "        \"\"\"\n",
        "        Define a Gaussian prior distribution with zero mean and unit variance.\n",
        "\n",
        "                Parameters:\n",
        "        M: [int]\n",
        "           Dimension of the latent space.\n",
        "        \"\"\"\n",
        "        super(GaussianPrior, self).__init__()\n",
        "        self.M = M\n",
        "        self.mean = nn.Parameter(torch.zeros(self.M), requires_grad=False)\n",
        "        self.std = nn.Parameter(torch.ones(self.M), requires_grad=False)\n",
        "\n",
        "    def forward(self):\n",
        "        \"\"\"\n",
        "        Return the prior distribution.\n",
        "\n",
        "        Returns:\n",
        "        prior: [torch.distributions.Distribution]\n",
        "        \"\"\"\n",
        "        return td.Independent(td.Normal(loc=self.mean, scale=self.std), 1)\n",
        "\n",
        "class GaussianEncoder(nn.Module):\n",
        "    def __init__(self, encoder_net):\n",
        "        \"\"\"\n",
        "        Define a Gaussian encoder distribution based on a given encoder network.\n",
        "\n",
        "        Parameters:\n",
        "        encoder_net: [torch.nn.Module]\n",
        "            The encoder network that takes a tensor of dimension\n",
        "            `(batch_size, feature_dim1, feature_dim2)` as input\n",
        "            and outputs a tensor of dimension `(batch_size, 2M)`,\n",
        "            where M is the dimension of the latent space.\n",
        "        \"\"\"\n",
        "        super(GaussianEncoder, self).__init__()\n",
        "        self.encoder_net = encoder_net\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Given a batch of input data, return a Gaussian distribution over the latent space.\n",
        "\n",
        "        Parameters:\n",
        "        x: [torch.Tensor]\n",
        "            A tensor of dimension `(batch_size, feature_dim1, feature_dim2)`.\n",
        "\n",
        "        Returns:\n",
        "        A Gaussian distribution with computed mean and standard deviation.\n",
        "        \"\"\"\n",
        "        mean, std = torch.chunk(self.encoder_net(x), 2, dim=-1)\n",
        "        return td.Independent(td.Normal(loc=mean, scale=torch.exp(std)), 1)\n",
        "\n",
        "        # Example:\n",
        "        # z = torch.randn(4, 10)  # Assume z is a tensor of shape [batch_size=4, 10]\n",
        "        # a, b = torch.chunk(z, 2, dim=-1)\n",
        "        # a and b will have shape [4, 5], as the tensor is split into two parts along the last dimension.\n",
        "def new_encoder():\n",
        "        encoder_net = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
        "            nn.Softmax(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
        "            nn.Softmax(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, 3, stride=2, padding=1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512, 2 * M),\n",
        "        )\n",
        "        return encoder_net\n",
        "class GaussianDecoder(nn.Module):\n",
        "    def __init__(self, decoder_net):\n",
        "        \"\"\"\n",
        "        Define a Gaussian decoder distribution based on a given decoder network.\n",
        "\n",
        "        Parameters:\n",
        "        decoder_net: [torch.nn.Module]\n",
        "            The decoder network that takes a tensor of dimension `(batch_size, M)`\n",
        "            as input, where M is the dimension of the latent space, and outputs a\n",
        "            tensor of dimension `(batch_size, feature_dim1, feature_dim2)`.\n",
        "        \"\"\"\n",
        "        super(GaussianDecoder, self).__init__()\n",
        "        self.decoder_net = decoder_net\n",
        "        # self.std = nn.Parameter(torch.ones(28, 28) * 0.5, requires_grad=True)\n",
        "        # In case you want to learn the standard deviation of the Gaussian.\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\"\n",
        "        Given a batch of latent variables, return a Gaussian distribution over the data space.\n",
        "\n",
        "        Parameters:\n",
        "        z: [torch.Tensor]\n",
        "            A tensor of dimension `(batch_size, M)`, where M is the dimension of the latent space.\n",
        "\n",
        "        Returns:\n",
        "        A Gaussian distribution with computed mean and a fixed standard deviation.\n",
        "        \"\"\"\n",
        "        means = self.decoder_net(z)\n",
        "        return td.Independent(td.Normal(loc=means, scale=1e-1), 3) #note the variance of decoder is fixed\n",
        "        # This defines a 784-dimensional independent normal distribution, where each dimension is independent.\n",
        "def new_decoder():\n",
        "        decoder_net = nn.Sequential(\n",
        "            nn.Linear(M, 512),\n",
        "            nn.Unflatten(-1, (32, 4, 4)),\n",
        "            nn.Softmax(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ConvTranspose2d(32, 32, 3, stride=2, padding=1, output_padding=0),\n",
        "            nn.Softmax(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Softmax(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
        "        )\n",
        "        return decoder_net"
      ],
      "metadata": {
        "id": "4Ewa8X8LuVcS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.VAE\n",
        "having changed the provided code ,so that for each mini-batch of data, we randomly sample a decoder and take a gradient step to optimize the ELBO\n"
      ],
      "metadata": {
        "id": "G3nNIR4xoM5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, prior, decoders, encoder):\n",
        "        \"\"\"\n",
        "        Variational Autoencoder (VAE) with multiple decoders.\n",
        "\n",
        "        Parameters:\n",
        "        prior: [torch.nn.Module]\n",
        "            The prior distribution over the latent space.\n",
        "        decoders: [list of torch.nn.Module]\n",
        "            A list containing multiple decoders.\n",
        "        encoder: [torch.nn.Module]\n",
        "            The encoder network that maps input data to a latent distribution.\n",
        "        \"\"\"\n",
        "        super(VAE, self).__init__()\n",
        "        self.prior = prior\n",
        "        self.decoders = nn.ModuleList(decoders)  # Use ModuleList to allow PyTorch to properly track parameters\n",
        "        self.encoder = encoder\n",
        "\n",
        "    def elbo(self, x, decoder_idx):\n",
        "        \"\"\"\n",
        "        Compute the Evidence Lower Bound (ELBO) for a given input and selected decoder.\n",
        "\n",
        "        Parameters:\n",
        "        x: [torch.Tensor]\n",
        "            The input data tensor.\n",
        "        decoder_idx: [int]\n",
        "            The index of the decoder to be used.\n",
        "\n",
        "        Returns:\n",
        "        The computed ELBO value.\n",
        "        \"\"\"\n",
        "        q = self.encoder(x)  # Encode input into a latent distribution\n",
        "        z = q.rsample()  # Sample from the latent distribution using the reparameterization trick\n",
        "        decoder = self.decoders[decoder_idx]  # Select the corresponding decoder\n",
        "\n",
        "        elbo = torch.mean(\n",
        "            decoder(z).log_prob(x) - q.log_prob(z) + self.prior().log_prob(z)\n",
        "        )  # Compute ELBO using the likelihood, posterior, and prior\n",
        "\n",
        "        return elbo\n",
        "\n",
        "    def sample(self, decoder_idx, n_samples=1):\n",
        "        \"\"\"\n",
        "        Generate samples from the specified decoder.\n",
        "\n",
        "        Parameters:\n",
        "        decoder_idx: [int]\n",
        "            The index of the decoder to be used.\n",
        "        n_samples: [int, default=1]\n",
        "            The number of samples to generate.\n",
        "\n",
        "        Returns:\n",
        "        A batch of generated samples.\n",
        "        \"\"\"\n",
        "        z = self.prior().sample(torch.Size([n_samples]))  # Sample from the prior distribution\n",
        "        decoder = self.decoders[decoder_idx]  # Select the corresponding decoder\n",
        "        return decoder(z).sample()  # Generate samples from the decoder\n",
        "\n",
        "    def forward(self, x, decoder_idx):\n",
        "        \"\"\"\n",
        "        Compute the negative ELBO for optimization.\n",
        "\n",
        "        Parameters:\n",
        "        x: [torch.Tensor]\n",
        "            The input data tensor.\n",
        "        decoder_idx: [int]\n",
        "            The index of the decoder to be used.\n",
        "\n",
        "        Returns:\n",
        "        The negative ELBO value.\n",
        "        \"\"\"\n",
        "        return -self.elbo(x, decoder_idx)\n"
      ],
      "metadata": {
        "id": "qKI-kp2_2Kaj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.Training vae\n"
      ],
      "metadata": {
        "id": "BGBEs4iMoT4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizers, data_loader, epochs, device):\n",
        "    num_decoders = len(model.decoders)\n",
        "    #one error\n",
        "    # have changed the code to fit with different number of decoders\n",
        "    total_epochs = epochs_per_decoder * num_decoders\n",
        "    num_steps = len(data_loader) * total_epochs\n",
        "    epoch = 0\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    def noise(x, std=0.05):\n",
        "        eps = std * torch.randn_like(x)\n",
        "        return torch.clamp(x + eps, min=0.0, max=1.0)\n",
        "\n",
        "    with tqdm(range(num_steps)) as pbar:\n",
        "        for step in pbar:\n",
        "            try:\n",
        "                x = next(iter(data_loader))[0]\n",
        "                x = noise(x.to(device))\n",
        "                model=model\n",
        "                idx = torch.randint(0, num_decoders, (1,)).item() #for each mini-batch of data, we randomly sample a decoder and take a gradient step to optimize the ELBO\n",
        "                optimizer = optimizers[idx]\n",
        "                optimizer.zero_grad()\n",
        "                loss = model(x, decoder_idx=idx) #correspond to the changed part in VAE\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                loss_val = loss.detach().cpu().item()\n",
        "                losses.append(loss_val)\n",
        "\n",
        "                if step % 5 == 0:\n",
        "                    pbar.set_description(\n",
        "                        f\"epoch={epoch}, step={step}, decoder={idx}, loss={loss_val:.1f}\"\n",
        "                    )\n",
        "                if (step + 1) % len(data_loader) == 0:\n",
        "                    epoch += 1\n",
        "            except KeyboardInterrupt:\n",
        "                print(f\"Stopped at epoch {epoch}, step {step}, loss {loss_val:.1f}\")\n",
        "                break\n",
        "\n",
        "    return losses\n"
      ],
      "metadata": {
        "id": "ORvNA3H97nNp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#because in part B we need 10 independent VAEs with  decoders 1,2,3, so I define a new train function\n",
        "#S is the number of the decoders\n",
        "def train_single_vae(seed, save_path, S, epochs_per_decoder):\n",
        "    set_seed(seed)\n",
        "    decoders = [GaussianDecoder(new_decoder()) for _ in range(S)]\n",
        "    #instantiating  S randomly initialized decoders\n",
        "    # note: set_seed(1001) only ensure the next time we run set_seed(1001), it's still the SAME randomly three decoders\n",
        "    # it will not destroy of the randomness of three different decoders\n",
        "    encoder = GaussianEncoder(new_encoder())\n",
        "    prior = GaussianPrior(M)\n",
        "\n",
        "    model = VAE(prior, decoders, encoder).to(device)\n",
        "   # I just use the learning rate provided\n",
        "    optimizers = [\n",
        "        torch.optim.Adam(\n",
        "            list(model.encoder.parameters()) + list(decoder.parameters()), lr=1e-3\n",
        "        )\n",
        "        for decoder in model.decoders\n",
        "    ]\n",
        "\n",
        "    losses = train(model, optimizers, mnist_train_loader, epochs_per_decoder, device)\n",
        "\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    plt.figure()\n",
        "    plt.plot(range(5000, len(losses)), losses[5000:])\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"ELBO Loss\")\n",
        "    plt.title(f\"Training Loss (Seed {seed}) [After 5000 Steps]\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path.replace(\".pt\", \"_loss.png\"))\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "DAYjbcQ05-xL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5 train ONE VAE single and ensemble"
      ],
      "metadata": {
        "id": "WcUldcuA2MxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# General training parameters\n",
        "epochs_per_decoder = 400  # In fact, if I use 300–400 total epochs for 3-decoder VAE, the performance is not well\n",
        "seed_base = 1000\n",
        "num_vaes = 3\n",
        "\n",
        "# Single decoder model\n",
        "num_decoders_single = 1\n",
        "experiments_folder_single = \"experiments/vae_single\"\n",
        "os.makedirs(experiments_folder_single, exist_ok=True)  # Create directory if it doesn't exist\n",
        "\n",
        "# Multiple decoder model\n",
        "num_decoders_ensemble = 3\n",
        "experiments_folder_ensemble = \"experiments/vae_ensemble\"\n",
        "os.makedirs(experiments_folder_ensemble, exist_ok=True)  # Create directory if it doesn't exist"
      ],
      "metadata": {
        "id": "thxtKogl6GGG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train single decoder model\n",
        "for i in range(num_vaes):  # Can be expanded to train multiple VAE models\n",
        "    seed = seed_base + i\n",
        "    save_path = f\"{experiments_folder_single}/sinmodel_seed{seed}.pt\"\n",
        "    print(f\"Training SINGLE model with seed {seed}...\")\n",
        "    train_single_vae(seed, save_path, num_decoders_single, epochs_per_decoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMUX6PRlYsmd",
        "outputId": "3ee8ae6e-d807-4d59-dde9-621b5d6eb1dc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SINGLE model with seed 1000...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/25600 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1739: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "epoch=399, step=25595, decoder=0, loss=32.3: 100%|██████████| 25600/25600 [03:21<00:00, 126.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SINGLE model with seed 1001...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=399, step=25595, decoder=0, loss=42.0: 100%|██████████| 25600/25600 [03:21<00:00, 127.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SINGLE model with seed 1002...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=399, step=25595, decoder=0, loss=-59.1: 100%|██████████| 25600/25600 [03:20<00:00, 127.58it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ensemble decoder model (one VAE with multiple decoders)\n",
        "for i in range(num_vaes):  # Can be expanded to train multiple models\n",
        "    seed = seed_base + i\n",
        "    save_path = f\"{experiments_folder_ensemble}/enmodel_seed{seed}.pt\"\n",
        "    print(f\"Training ENSEMBLE model with seed {seed}...\")\n",
        "    train_single_vae(seed, save_path, num_decoders_ensemble, epochs_per_decoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVNfwkEBkewT",
        "outputId": "f8eeeb10-df42-40bf-ea43-9dc4fb77f590"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ENSEMBLE model with seed 1000...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=1199, step=76795, decoder=2, loss=-143.6: 100%|██████████| 76800/76800 [10:00<00:00, 127.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ENSEMBLE model with seed 1001...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=1199, step=76795, decoder=2, loss=48.9: 100%|██████████| 76800/76800 [10:03<00:00, 127.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ENSEMBLE model with seed 1002...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=1199, step=76795, decoder=2, loss=-235.8: 100%|██████████| 76800/76800 [10:03<00:00, 127.16it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After training, download the model_seed.pt, next time directly load the model\n",
        "\n",
        "# Parameters\n",
        "M = latent_dim\n",
        "num_classes = 3\n",
        "\n",
        "# Load single decoder models\n",
        "vae_single_list = []\n",
        "for i in range(num_vaes):\n",
        "    seed = seed_base + i\n",
        "    decoders_single = [GaussianDecoder(new_decoder())]\n",
        "    encoder_single = GaussianEncoder(new_encoder())\n",
        "    prior_single = GaussianPrior(M)\n",
        "    vae_single = VAE(prior_single, decoders_single, encoder_single).to(device)\n",
        "    vae_single.load_state_dict(torch.load(f\"{experiments_folder_single}/sinmodel_seed{seed}.pt\"))\n",
        "    vae_single_list.append(vae_single)  # Store as vae_single_list[0], [1], [2] = vae_single1, 2, 3\n",
        "\n",
        "# Load ensemble decoder models\n",
        "vae_ensemble_list = []\n",
        "for i in range(num_vaes):\n",
        "    seed = seed_base + i\n",
        "    decoders_ensemble = [GaussianDecoder(new_decoder()) for _ in range(num_decoders_ensemble)]\n",
        "    encoder_ensemble = GaussianEncoder(new_encoder())\n",
        "    prior_ensemble = GaussianPrior(M)\n",
        "    vae_ensemble = VAE(prior_ensemble, decoders_ensemble, encoder_ensemble).to(device)\n",
        "    vae_ensemble.load_state_dict(torch.load(f\"{experiments_folder_ensemble}/enmodel_seed{seed}.pt\"))\n",
        "    vae_ensemble_list.append(vae_ensemble)  # Store as vae_ensemble_list[0], [1], [2] = vae_ensemble1, 2, 3"
      ],
      "metadata": {
        "id": "ps_jve53nDOX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.1 latent space:\n",
        "not relevant, just for checking whether the model encode the right latent space"
      ],
      "metadata": {
        "id": "TSCKksdsmKdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_latent_space(model, dataloader, num_classes, save_path):\n",
        "    model.eval()\n",
        "    zs, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x = x.to(device)\n",
        "            z = model.encoder(x).base_dist.loc\n",
        "            zs.append(z.cpu())\n",
        "            labels.append(y)\n",
        "    zs = torch.cat(zs, dim=0).numpy()\n",
        "    labels = torch.cat(labels, dim=0).numpy()\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    for i in range(num_classes):\n",
        "        idx = labels == i\n",
        "        plt.scatter(zs[idx, 0], zs[idx, 1], s=5, alpha=0.6, label=f\"Class {i}\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Latent Space\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "j1qxt-EBsYcv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the latent space for each single decoder model\n",
        "for i, vae_single in enumerate(vae_single_list):\n",
        "    visualize_latent_space(\n",
        "        vae_single,\n",
        "        mnist_test_loader,\n",
        "        num_classes=num_classes,\n",
        "        save_path=f\"latent_space_single_{i+1}.png\"\n",
        "    )\n",
        "\n",
        "# Visualize the latent space for each ensemble decoder model\n",
        "for i, vae_ensemble in enumerate(vae_ensemble_list):\n",
        "    visualize_latent_space(\n",
        "        vae_ensemble,\n",
        "        mnist_test_loader,\n",
        "        num_classes=num_classes,\n",
        "        save_path=f\"latent_space_ensemble_{i+1}.png\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "XRKEVNIIsbOi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45438cb3-c7e8-412a-e72c-668a9c73c78b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1739: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.2 sample\n",
        "not relevant to this task, just for checking the quality of VAE\n"
      ],
      "metadata": {
        "id": "qYydvSnInuB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Single decoder outputs ===\n",
        "experiments_single = \"experiments/vae_single\"\n",
        "outputs_single = \"experiments/vae_single_outputs\"\n",
        "os.makedirs(outputs_single, exist_ok=True)\n",
        "\n",
        "for i, vae_single in enumerate(vae_single_list):\n",
        "    seed = seed_base + i\n",
        "    vae_single.eval()  # Set model to evaluation mode\n",
        "\n",
        "    out_dir_single = f\"{outputs_single}/vae_seed{seed}\"\n",
        "    os.makedirs(out_dir_single, exist_ok=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Sampling (only one decoder)\n",
        "        samples = vae_single.sample(decoder_idx=0, n_samples=64).cpu()\n",
        "        save_image(samples.view(64, 1, 28, 28), f\"{out_dir_single}/samples_decoder0.png\")\n",
        "\n",
        "        # Reconstruction\n",
        "        data = next(iter(mnist_test_loader))[0].to(device)\n",
        "        z = vae_single.encoder(data).mean\n",
        "        recon = vae_single.decoders[0](z).mean\n",
        "        save_image(\n",
        "            torch.cat([data.cpu(), recon.cpu()], dim=0),\n",
        "            f\"{out_dir_single}/reconstruction_decoder0.png\"\n",
        "        )"
      ],
      "metadata": {
        "id": "8m9fnm5dukRX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Ensemble decoder outputs ===\n",
        "outputs_ensemble = \"experiments/vae_ensemble_outputs\"\n",
        "os.makedirs(outputs_ensemble, exist_ok=True)\n",
        "\n",
        "for i, vae_ensemble in enumerate(vae_ensemble_list):\n",
        "    seed = seed_base + i\n",
        "    vae_ensemble.eval()\n",
        "\n",
        "    out_dir_ensemble = f\"{outputs_ensemble}/vae_seed{seed}\"\n",
        "    os.makedirs(out_dir_ensemble, exist_ok=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Sampling from each decoder\n",
        "        for j in range(num_decoders_ensemble):\n",
        "            samples = vae_ensemble.sample(decoder_idx=j, n_samples=64).cpu()\n",
        "            save_image(samples.view(64, 1, 28, 28), f\"{out_dir_ensemble}/samples_decoder{j}.png\")\n",
        "\n",
        "        # Reconstruction using each decoder\n",
        "        data = next(iter(mnist_test_loader))[0].to(device)\n",
        "        z = vae_ensemble.encoder(data).mean\n",
        "        for j in range(num_decoders_ensemble):\n",
        "            recon = vae_ensemble.decoders[j](z).mean\n",
        "            save_image(\n",
        "                torch.cat([data.cpu(), recon.cpu()], dim=0),\n",
        "                f\"{out_dir_ensemble}/reconstruction_decoder{j}.png\"\n",
        "            )"
      ],
      "metadata": {
        "id": "NhfTtVxourQg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.3 elbo not relevant to this task\n"
      ],
      "metadata": {
        "id": "ZsfgWdGVsNBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "num_vaes =1\n",
        "experiments_folder = \"experiments/vae_retrain_seeds\"\n",
        "\n",
        "for vae_idx in range(num_vaes):\n",
        "    seed = 1000 + vae_idx\n",
        "    model_path = f\"{experiments_folder}/model_seed{seed}.pt\"\n",
        "\n",
        "\n",
        "    decoders = [GaussianDecoder(new_decoder()) for _ in range(num_decoders)]\n",
        "    encoder = GaussianEncoder(new_encoder())\n",
        "    prior = GaussianPrior(M)\n",
        "\n",
        "    model = VAE(prior, decoders, encoder).to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    elbos_per_decoder = [[] for _ in range(num_decoders)]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, _ in mnist_test_loader:\n",
        "            x = x.to(device)\n",
        "            for i in range(num_decoders):\n",
        "                elbo = model.elbo(x, decoder_idx=i)\n",
        "                elbos_per_decoder[i].append(elbo)\n",
        "\n",
        "\n",
        "    print(f\"\\nVAE model with seed {seed}:\")\n",
        "    for i in range(num_decoders):\n",
        "        mean_elbo = torch.tensor(elbos_per_decoder[i]).mean()\n",
        "        print(f\"  Decoder {i} mean test ELBO: {mean_elbo.item():.4f}\")\n",
        "'''\n"
      ],
      "metadata": {
        "id": "sPMF4FLSty75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "a5e85512-c66f-4fb6-b721-e4b038305319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnum_vaes =1\\nexperiments_folder = \"experiments/vae_retrain_seeds\"\\n\\nfor vae_idx in range(num_vaes):\\n    seed = 1000 + vae_idx\\n    model_path = f\"{experiments_folder}/model_seed{seed}.pt\"\\n\\n    # 初始化结构（必须一致）\\n    decoders = [GaussianDecoder(new_decoder()) for _ in range(num_decoders)]\\n    encoder = GaussianEncoder(new_encoder())\\n    prior = GaussianPrior(M)\\n\\n    model = VAE(prior, decoders, encoder).to(device)\\n    model.load_state_dict(torch.load(model_path, map_location=device))\\n    model.eval()\\n\\n    elbos_per_decoder = [[] for _ in range(num_decoders)]\\n\\n    with torch.no_grad():\\n        for x, _ in mnist_test_loader:\\n            x = x.to(device)\\n            for i in range(num_decoders):\\n                elbo = model.elbo(x, decoder_idx=i)\\n                elbos_per_decoder[i].append(elbo)\\n\\n    # 输出每个 decoder 的 mean ELBO\\n    print(f\"\\nVAE model with seed {seed}:\")\\n    for i in range(num_decoders):\\n        mean_elbo = torch.tensor(elbos_per_decoder[i]).mean()\\n        print(f\"  Decoder {i} mean test ELBO: {mean_elbo.item():.4f}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6 cruve and energy\n"
      ],
      "metadata": {
        "id": "Z7_VxVHhFZzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.1 cubiccurve\n"
      ],
      "metadata": {
        "id": "j_8WxRNksmWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CubicCurve(nn.Module):\n",
        "    def __init__(self, c0, c1):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        c0: torch.Tensor, start point (D,)\n",
        "        c1: torch.Tensor, end point (D,)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Use GPU if available\n",
        "        self.T = 128  # Number of segments\n",
        "        self.D = c0.shape[0]  # Dimensionality of the curve\n",
        "\n",
        "        self.register_buffer(\"c0\", c0.to(self.device))  # Register start point as buffer (not trainable)\n",
        "        self.register_buffer(\"c1\", c1.to(self.device))  # Register end point as buffer (not trainable)\n",
        "\n",
        "        # Initialize intermediate control points to form a straight line between c0 and c1\n",
        "        # This ensures the initial curve is a straight line before optimization\n",
        "        intermediate_points = torch.stack([\n",
        "            c0 + (c1 - c0) * (i + 1) / self.T  # Equally spaced points along the line from c0 to c1\n",
        "            for i in range(self.T - 1)\n",
        "        ], dim=0).to(self.device)  # Shape: (T-1, D)\n",
        "\n",
        "        self.c_t = nn.Parameter(intermediate_points)  # Make intermediate control points trainable\n",
        "\n",
        "        # Create a uniform grid of time values from 0 to 1 with T+1 points\n",
        "        self.register_buffer(\"t_grid\", torch.linspace(0, 1, self.T + 1, device=self.device))\n",
        "\n",
        "    def forward(self, t):\n",
        "        \"\"\"\n",
        "        Compute the value of the piecewise linear curve at position t\n",
        "\n",
        "        Parameters:\n",
        "        t: torch.Tensor, shape (...,), range [0,1]\n",
        "\n",
        "        Returns:\n",
        "        torch.Tensor, shape (...,D)\n",
        "\n",
        "        Each segment is linearly interpolated between two control points:\n",
        "        Given control points p0 and p1, and a scalar alpha ∈ [0, 1],\n",
        "        interpolation formula is: p(t) = (1 - alpha) * p0 + alpha * p1\n",
        "        where alpha = (t - t0) / (t1 - t0)\n",
        "        \"\"\"\n",
        "        t = t.to(self.device)  # Ensure t is on the correct device\n",
        "\n",
        "        # Concatenate all control points: [c0, ..., c_t, ..., c1], shape (T+1, D)\n",
        "        control_points = torch.cat([self.c0.unsqueeze(0), self.c_t, self.c1.unsqueeze(0)], dim=0)\n",
        "\n",
        "        # For each t, find the corresponding segment index in t_grid\n",
        "        idx = torch.searchsorted(self.t_grid, t, right=True) - 1  # Get left index of segment\n",
        "        idx = idx.clamp(0, self.T - 1)  # Clamp to valid range\n",
        "\n",
        "        # Get start and end times of the segment\n",
        "        t0, t1 = self.t_grid[idx], self.t_grid[idx + 1]\n",
        "\n",
        "        # Compute alpha for interpolation within the segment\n",
        "        alpha = (t - t0) / (t1 - t0 + 1e-8)  # Normalize t within [t0, t1]\n",
        "\n",
        "        # Get the corresponding control points\n",
        "        c0, c1 = control_points[idx], control_points[idx + 1]\n",
        "\n",
        "        # Perform linear interpolation:\n",
        "        # p(t) = (1 - alpha) * c0 + alpha * c1\n",
        "        # This is derived from:\n",
        "        # If t ∈ [t_i, t_{i+1}], then:\n",
        "        #     p(t) = c_i + (c_{i+1} - c_i) * (t - t_i) / (t_{i+1} - t_i)\n",
        "        #          = (1 - alpha) * c_i + alpha * c_{i+1}\n",
        "        return (1 - alpha.unsqueeze(-1)) * c0 + alpha.unsqueeze(-1) * c1  # (...,D)\n"
      ],
      "metadata": {
        "id": "V7r1Tut2FzsN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create start and end points\n",
        "c0 = torch.tensor([0.0, 0.0])\n",
        "c1 = torch.tensor([1.0, 1.0])\n",
        "\n",
        "# Instantiate the curve object\n",
        "curve = CubicCurve(c0, c1)\n",
        "\n",
        "# Example t values (must be in the range [0,1])\n",
        "t_sample = torch.tensor([0.0, 0.1, 0.5, 0.9, 1.0])\n",
        "\n",
        "# Compute corresponding values on the curve\n",
        "curve_values = curve(t_sample)\n",
        "\n",
        "print(\"t_sample shape:\", t_sample.shape)  # Shape of input t\n",
        "print(\"curve_values shape:\", curve_values.shape)  # Shape of output curve values\n",
        "print(\"curve_values content:\", curve_values)  # Output values at the sampled t positions\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cV48WpmwO2-U",
        "outputId": "4fb9ff6c-6c25-4369-cc05-b78905049d4b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t_sample shape: torch.Size([5])\n",
            "curve_values shape: torch.Size([5, 2])\n",
            "curve_values content: tensor([[0.0000, 0.0000],\n",
            "        [0.1000, 0.1000],\n",
            "        [0.5000, 0.5000],\n",
            "        [0.9000, 0.9000],\n",
            "        [1.0000, 1.0000]], device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create start and end points\n",
        "c0 = torch.tensor([0.0, 0.0])\n",
        "c1 = torch.tensor([1.0, 1.0])\n",
        "\n",
        "# Initialize the curve object\n",
        "curve = CubicCurve(c0, c1)\n",
        "\n",
        "# Sample t values uniformly from 0 to 1\n",
        "t_vals = torch.linspace(0, 1, 256)\n",
        "\n",
        "# Compute curve points without gradient tracking\n",
        "with torch.no_grad():\n",
        "    points = curve(t_vals)  # Shape: (256, 2)\n",
        "\n",
        "# Extract dimension 0 and move to CPU\n",
        "y_vals = points[:, 0].cpu()\n",
        "\n",
        "# Plot the curve for dimension 0 as a function of t\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(t_vals.cpu(), y_vals, label='Dim 0')\n",
        "plt.title(\"Cubic Curve - Dimension 0\")\n",
        "plt.xlabel(\"t\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "f58pfhlzUN3u",
        "outputId": "520a5190-5c55-4696-cbcd-93bcec8cf316"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.2 compute_energy\n",
        "\n",
        "$$\n",
        "\\mathcal{E}[\\gamma] \\approx \\sum_{t=0}^{T-1} \\mathbb{E}_{\\theta, \\theta' \\sim q(\\theta) q(\\theta)}\n",
        "\\left[ \\left\\| f_{\\theta} (\\gamma(t + \\frac{1}{T})) - f_{\\theta'} (\\gamma(t / T)) \\right\\|^2 \\right]\n",
        "$$\n",
        "\n",
        "$f_{\\theta}$ $f_{\\theta'}$ denotes deoder ensemble members drawn uniformly"
      ],
      "metadata": {
        "id": "FuBRA6srsq2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_curve_energy(curve, decoders, T=16, num_samples=1, fixed_indices=None, device='cuda'):\n",
        "    \"\"\"\n",
        "    Compute the energy of a curve using fixed decoder indices to ensure the objective function remains consistent.\n",
        "\n",
        "    Parameters:\n",
        "    - curve: An instance of CubicCurve\n",
        "    - decoders: List of decoder modules\n",
        "    - T: Number of time steps, default is 16\n",
        "    - num_samples: Number of Monte Carlo samples, default is 1\n",
        "    - fixed_indices: Pre-fixed decoder indices [(idx1_t0, idx2_t0), (idx1_t1, idx2_t1), ...]\n",
        "    - device: Computing device, default is 'cuda'\n",
        "\n",
        "    Returns:\n",
        "    - Scalar energy value\n",
        "    \"\"\"\n",
        "    total_energy = 0.0  # Accumulate energy over all time steps\n",
        "\n",
        "    for i in range(T):\n",
        "        t0 = torch.tensor([i / T], device=device, dtype=torch.float32)\n",
        "        t1 = torch.tensor([(i + 1) / T], device=device, dtype=torch.float32)\n",
        "\n",
        "        x0 = curve(t0)  # γ(t0), shape [1, d]\n",
        "        x1 = curve(t1)  # γ(t1)\n",
        "\n",
        "        energy = 0.0  # Energy for the current time step\n",
        "\n",
        "        for _ in range(num_samples):\n",
        "            idx1, idx2 = fixed_indices[i]  # Retrieve fixed indices\n",
        "\n",
        "            # **Compute only the required decoder outputs**\n",
        "            sampled_mean_x0 = decoders[idx1](x0).mean  # Directly compute the mean for idx1\n",
        "            sampled_mean_x1 = decoders[idx2](x1).mean  # Directly compute the mean for idx2\n",
        "\n",
        "            # Compute L2 norm\n",
        "            energy += torch.norm(sampled_mean_x1 - sampled_mean_x0, p=2)\n",
        "\n",
        "        # Take Monte Carlo average, negligible when num_samples = 1\n",
        "        total_energy += energy / num_samples\n",
        "\n",
        "    return total_energy  # Return total energy\n"
      ],
      "metadata": {
        "id": "bogT4psFQ0Y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.3 optimize_geodesics"
      ],
      "metadata": {
        "id": "6e6UxF2PO3Br"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_geodesic(c0, c1, decoders, T=16, steps=500, lr=1e-2, device='cuda',\n",
        "                      early_stopping_n=100, early_stopping_delta=1e-4):\n",
        "    \"\"\"\n",
        "    Optimize a geodesic curve while ensuring the objective function remains unchanged during optimization.\n",
        "\n",
        "    Parameters:\n",
        "    - c0: Starting point\n",
        "    - c1: Endpoint\n",
        "    - decoders: List of decoder modules\n",
        "    - T: Number of time steps\n",
        "    - steps: Number of optimization iterations\n",
        "    - lr: Learning rate\n",
        "    - device: Computing device\n",
        "    - early_stopping_n: Number of steps to check for early stopping\n",
        "    - early_stopping_delta: Minimum required improvement to continue\n",
        "\n",
        "    Returns:\n",
        "    - Optimized curve\n",
        "    - Logged energy values\n",
        "    \"\"\"\n",
        "    curve = CubicCurve(c0, c1).to(device)  # Initialize the curve\n",
        "    optimizer = torch.optim.Adam(curve.parameters(), lr=lr)  # Adam optimizer\n",
        "    energy_log = []  # Store energy values\n",
        "\n",
        "    # **Pre-generate fixed decoder indices**\n",
        "    fixed_indices = [(torch.randint(0, len(decoders), (1,), device=device).item(),\n",
        "                      torch.randint(0, len(decoders), (1,), device=device).item())\n",
        "                     for _ in range(T)]\n",
        "\n",
        "    best_energy = float('inf')\n",
        "    no_improve_count = 0  # Counter for early stopping\n",
        "\n",
        "    with tqdm(range(steps)) as pbar:\n",
        "        for step in pbar:\n",
        "            optimizer.zero_grad()  # Clear gradients\n",
        "            energy = compute_curve_energy(curve, decoders, T=T, fixed_indices=fixed_indices, device=device)  # Compute energy\n",
        "            energy.backward()  # Backpropagation\n",
        "            optimizer.step()  # Update parameters\n",
        "\n",
        "            energy_value = energy.item()\n",
        "            energy_log.append(energy_value)  # Store energy value\n",
        "\n",
        "            # Early Stopping Logic\n",
        "            if energy_value < best_energy - early_stopping_delta:\n",
        "                best_energy = energy_value\n",
        "                no_improve_count = 0  # Reset counter\n",
        "            else:\n",
        "                no_improve_count += 1\n",
        "\n",
        "            if no_improve_count >= early_stopping_n:\n",
        "                print(f\"Early stopping at step {step}, energy: {energy_value:.6f}\")\n",
        "                break  # Stop training if no improvement\n",
        "\n",
        "            # Update progress bar\n",
        "            pbar.set_description(f\"Energy: {energy_value:.6f}\")\n",
        "\n",
        "    return curve, energy_log\n"
      ],
      "metadata": {
        "id": "JqEQ2nKLotZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.plot  25 random latent_varibale pairs\n"
      ],
      "metadata": {
        "id": "LE52pKwas5TJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7.1 data preparation"
      ],
      "metadata": {
        "id": "uKh_3_IruXFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_pairs = 5  # Number of image pairs, for testing use 5, should use 25 in full runs\n",
        "\n",
        "# ====== Extract All Test Images ======\n",
        "test_images = []\n",
        "test_labels = []\n",
        "for x, y in mnist_test_loader:\n",
        "    test_images.append(x)\n",
        "    test_labels.append(y)\n",
        "test_images = torch.cat(test_images, dim=0)  # Shape: [N, 1, 28, 28]\n",
        "test_labels = torch.cat(test_labels, dim=0)\n",
        "\n",
        "# ====== Randomly Sample Image Pairs ======\n",
        "N = test_images.shape[0]\n",
        "indices = random.sample(range(N), 2 * num_pairs)  # Sample 2 * num_pairs indices\n",
        "x_pairs = torch.stack([\n",
        "    torch.stack([test_images[indices[i]], test_images[indices[i + 1]]], dim=0)\n",
        "    for i in range(0, 2 * num_pairs, 2)\n",
        "])  # Shape: [num_pairs, 2, 1, 28, 28]\n",
        "\n",
        "# ====== Encode Image Pairs into Latent Space ======\n",
        "z_pairs_single_list = []     # Each element corresponds to one VAE (single)\n",
        "z_pairs_ensemble_list = []   # Each element corresponds to one VAE (ensemble)\n",
        "\n",
        "# Process all single decoder VAEs\n",
        "for vae_single in vae_single_list:\n",
        "    vae_single.eval()\n",
        "    z_pairs = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(num_pairs):\n",
        "            x0 = x_pairs[i, 0].to(device)\n",
        "            x1 = x_pairs[i, 1].to(device)\n",
        "            z0 = vae_single.encoder(x0.unsqueeze(0)).base_dist.loc.squeeze(0)\n",
        "            z1 = vae_single.encoder(x1.unsqueeze(0)).base_dist.loc.squeeze(0)\n",
        "            z_pairs.append(torch.stack([z0, z1], dim=0))\n",
        "    z_pairs_single_list.append(torch.stack(z_pairs))  # Shape: [num_pairs, 2, latent_dim]\n",
        "\n",
        "# Process all ensemble decoder VAEs\n",
        "for vae_ensemble in vae_ensemble_list:\n",
        "    vae_ensemble.eval()\n",
        "    z_pairs = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(num_pairs):\n",
        "            x0 = x_pairs[i, 0].to(device)\n",
        "            x1 = x_pairs[i, 1].to(device)\n",
        "            z0 = vae_ensemble.encoder(x0.unsqueeze(0)).base_dist.loc.squeeze(0)\n",
        "            z1 = vae_ensemble.encoder(x1.unsqueeze(0)).base_dist.loc.squeeze(0)\n",
        "            z_pairs.append(torch.stack([z0, z1], dim=0))\n",
        "    z_pairs_ensemble_list.append(torch.stack(z_pairs))  # Shape: [num_pairs, 2, latent_dim]\n"
      ],
      "metadata": {
        "id": "u66xdqOMJYLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Parameter Settings ======\n",
        "T = 128  # Number of segments for energy calculation,T=20, maybe too small? but too large require longer time\n",
        "steps = 2000  # Number of optimization iterations, some points still don't decrease,maybe we should use early stopping\n",
        "lr = 5*1e-3 # Learning rate or should it be tuned larger?\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Set device"
      ],
      "metadata": {
        "id": "OeH2LbBXuVXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "curves_single = []\n",
        "energy_logs_single = []\n",
        "\n",
        "for i in range(num_pairs):\n",
        "    c0, c1 = z_pairs_single[i, 0].to(device), z_pairs_single[i, 1].to(device)\n",
        "    curve, energy_log = optimize_geodesic(c0, c1,\n",
        "                                          decoders=[vae_single.decoders[0]],  # only use one decoder\n",
        "                                          T=T,\n",
        "                                          steps=steps,\n",
        "                                          lr=lr,\n",
        "                                          device=device,\n",
        "                                          early_stopping_n=50,  # patience\n",
        "                                          early_stopping_delta=1e-3)\n",
        "\n",
        "    curves_single.append(curve)  # the curev after optimizing\n",
        "    energy_logs_single.append(energy_log)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1GQ86eCKPY4",
        "outputId": "702b67d5-80bb-472c-f63d-c578cd90731d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Energy: 16.435904:   7%|▋         | 137/2000 [01:30<20:30,  1.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at step 137, energy: 16.435812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Energy: 14.798503:  10%|█         | 210/2000 [02:17<19:30,  1.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at step 210, energy: 14.798399\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Energy: 8.002676:   2%|▎         | 50/2000 [00:33<21:30,  1.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at step 50, energy: 8.002597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Energy: 20.770960: 100%|██████████| 2000/2000 [21:44<00:00,  1.53it/s]\n",
            "Energy: 17.474449:  93%|█████████▎| 1853/2000 [20:07<01:35,  1.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at step 1853, energy: 17.474434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('results_single', exist_ok=True)  # Create base directory if it doesn't exist\n",
        "\n",
        "all_curves_single = []         # List to store curve lists per model\n",
        "all_energy_logs_single = []    # List to store energy log lists per model\n",
        "\n",
        "for model_idx, z_pairs_single in enumerate(z_pairs_single_list):  # Loop over all single decoder models\n",
        "    curves_single = []        # Curves for this model\n",
        "    energy_logs_single = []   # Energy logs for this model\n",
        "\n",
        "    model_result_dir = f'results_single/model_{model_idx+1}'\n",
        "    os.makedirs(model_result_dir, exist_ok=True)\n",
        "\n",
        "    for i in range(num_pairs):\n",
        "        curve_path = f'{model_result_dir}/curve_single_{i+1}.pt'       # Path to save/load curve\n",
        "        energy_log_path = f'{model_result_dir}/energylog_single_{i+1}.pt'  # Path to save/load energy log\n",
        "\n",
        "        if os.path.exists(curve_path) and os.path.exists(energy_log_path):\n",
        "            # Load from existing files\n",
        "            loaded_curve = torch.load(curve_path, map_location=device, weights_only=False)\n",
        "            loaded_energy_log = torch.load(energy_log_path, map_location=device, weights_only=False)\n",
        "        else:\n",
        "            # Optimize and save\n",
        "            c0, c1 = z_pairs_single[i, 0].to(device), z_pairs_single[i, 1].to(device)\n",
        "            loaded_curve, loaded_energy_log = optimize_geodesic(\n",
        "                c0, c1,\n",
        "                decoders=[vae_single_list[model_idx].decoders[0]],  # Use decoder 0 from this model\n",
        "                T=T,\n",
        "                steps=steps,\n",
        "                lr=lr,\n",
        "                device=device,\n",
        "                early_stopping_n=50,\n",
        "                early_stopping_delta=1e-3\n",
        "            )\n",
        "            torch.save(loaded_curve, curve_path)\n",
        "            torch.save(loaded_energy_log, energy_log_path)\n",
        "\n",
        "        curves_single.append(loaded_curve)\n",
        "        energy_logs_single.append(loaded_energy_log)\n",
        "\n",
        "    all_curves_single.append(curves_single)\n",
        "    all_energy_logs_single.append(energy_logs_single)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "I9lVg9MOjcrA",
        "outputId": "8926968a-153d-4834-cbbf-f566dcba7b4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2000 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1739: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "Energy: 8.004173:   2%|▎         | 50/2000 [00:33<21:50,  1.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at step 50, energy: 8.004137\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Energy: 22.798315:  10%|▉         | 198/2000 [02:08<19:33,  1.54it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-131-9bf53a5a0c63>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# 文件不存在，重新训练并保存\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mc0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz_pairs_single\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_pairs_single\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         loaded_curve, loaded_energy_log = optimize_geodesic(\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mc0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mdecoders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvae_single\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-110-dc5a4c59cd2e>\u001b[0m in \u001b[0;36moptimize_geodesic\u001b[0;34m(c0, c1, decoders, T, steps, lr, device, early_stopping_n, early_stopping_delta)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Clear gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_curve_energy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfixed_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute energy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0menergy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In fact,different points has different decreasing covergence speed, so I guess we can use early stopping, setting max=5000, if within patience=50 steps , the energy is basically the same, then early stop.\n",
        "\n",
        "just refer to the orginial paper (also use early stopping)\n",
        "https://github.com/mustass/ensertainty/blob/main/configs/inference/ensemble_geodesics.yaml"
      ],
      "metadata": {
        "id": "zHTCa9_w2-Ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T = 128\n",
        "steps = 200\n",
        "lr = 5 * 1e-2\n",
        "\n",
        "os.makedirs('results_ensemble', exist_ok=True)  # Base directory\n",
        "\n",
        "all_curves_ensemble = []         # List to store curves per ensemble model\n",
        "all_energy_logs_ensemble = []    # List to store energy logs per ensemble model\n",
        "\n",
        "for model_idx, z_pairs_ensemble in enumerate(z_pairs_ensemble_list):  # Loop over all ensemble models\n",
        "    curves_ensemble = []        # Curves for this model\n",
        "    energy_logs_ensemble = []   # Energy logs for this model\n",
        "\n",
        "    model_result_dir = f'results_ensemble/model_{model_idx+1}'\n",
        "    os.makedirs(model_result_dir, exist_ok=True)\n",
        "\n",
        "    for i in range(num_pairs):\n",
        "        curve_path = f'{model_result_dir}/curve_ensemble_{i+1}.pt'\n",
        "        energy_log_path = f'{model_result_dir}/energylog_ensemble_{i+1}.pt'\n",
        "\n",
        "        if os.path.exists(curve_path) and os.path.exists(energy_log_path):\n",
        "            # Load existing results\n",
        "            loaded_curve = torch.load(curve_path, map_location=device, weights_only=False)\n",
        "            loaded_energy_log = torch.load(energy_log_path, map_location=device, weights_only=False)\n",
        "        else:\n",
        "            # Run optimization and save\n",
        "            c0, c1 = z_pairs_ensemble[i, 0].to(device), z_pairs_ensemble[i, 1].to(device)\n",
        "            loaded_curve, loaded_energy_log = optimize_geodesic(\n",
        "                c0, c1,\n",
        "                decoders=vae_ensemble_list[model_idx].decoders,\n",
        "                T=T,\n",
        "                steps=steps,\n",
        "                lr=lr,\n",
        "                device=device,\n",
        "                early_stopping_n=50,\n",
        "                early_stopping_delta=1e-3\n",
        "            )\n",
        "            torch.save(loaded_curve, curve_path)\n",
        "            torch.save(loaded_energy_log, energy_log_path)\n",
        "\n",
        "        curves_ensemble.append(loaded_curve)\n",
        "        energy_logs_ensemble.append(loaded_energy_log)\n",
        "\n",
        "    all_curves_ensemble.append(curves_ensemble)\n",
        "    all_energy_logs_ensemble.append(energy_logs_ensemble)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "chY0BkP3joKg",
        "outputId": "1d61b879-dd21-4927-fada-2ff7592363e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/200 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1739: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "Energy: 101.352852: 100%|██████████| 200/200 [02:10<00:00,  1.53it/s]\n",
            "Energy: 84.366440:  14%|█▍        | 29/200 [00:19<01:53,  1.51it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-138-5ebd95a5e1fe>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mc0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz_pairs_ensemble\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_pairs_ensemble\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         loaded_curve, loaded_energy_log = optimize_geodesic(\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mc0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mdecoders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvae_ensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-110-dc5a4c59cd2e>\u001b[0m in \u001b[0;36moptimize_geodesic\u001b[0;34m(c0, c1, decoders, T, steps, lr, device, early_stopping_n, early_stopping_delta)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Clear gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_curve_energy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfixed_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute energy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0menergy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-109-cfeb8de7cde9>\u001b[0m in \u001b[0;36mcompute_curve_energy\u001b[0;34m(curve, decoders, T, num_samples, fixed_indices, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# γ(t0), shape [1, d]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# γ(t1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m  \u001b[0;31m# Energy for the current time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-108-310672ee728e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mc0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontrol_points\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol_points\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mc0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mc1\u001b[0m  \u001b[0;31m# (...,D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.2 plot the energy curve\n",
        "testing the optimizer"
      ],
      "metadata": {
        "id": "BF0XsA25wUnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_energy_logs_per_pair(energy_logs, decoder_count, model_idx, base_folder=\"energy_logs\"):\n",
        "    \"\"\"\n",
        "    Plot the energy variation curve for each pair of points and save them in separate folders per model.\n",
        "\n",
        "    Parameters:\n",
        "    - energy_logs: List of energy logs for one model (list of length num_pairs).\n",
        "    - decoder_count: int, number of decoders used in the model (1 or >1).\n",
        "    - model_idx: int, index of the current model (starting from 0).\n",
        "    - base_folder: str, root directory for saving the plots.\n",
        "    \"\"\"\n",
        "    folder = os.path.join(base_folder, f\"vae_d{decoder_count}\", f\"model_{model_idx+1}\")\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "    for pair_idx, log in enumerate(energy_logs):\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(log)\n",
        "        plt.xlabel(\"Optimization Step\")\n",
        "        plt.ylabel(\"Energy\")\n",
        "        plt.title(f\"Geodesic Energy (Decoder={decoder_count}, Model={model_idx+1}, Pair={pair_idx})\")\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        save_path = os.path.join(folder, f\"pair_{pair_idx}.png\")\n",
        "        plt.savefig(save_path, dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ICxaqv4a0BDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot for all single decoder models\n",
        "for model_idx, energy_logs in enumerate(all_energy_logs_single):\n",
        "    plot_energy_logs_per_pair(energy_logs, decoder_count=1, model_idx=model_idx)"
      ],
      "metadata": {
        "id": "X7A7-Uzz0EVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot for all ensemble decoder models\n",
        "for model_idx, energy_logs in enumerate(all_energy_logs_ensemble):\n",
        "    plot_energy_logs_per_pair(energy_logs, decoder_count=3, model_idx=model_idx)"
      ],
      "metadata": {
        "id": "Ejd0JG3A2wON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7.3 plot the required comparision plot of geodesics"
      ],
      "metadata": {
        "id": "ee_1ltsxxKHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Encode all test data for all models ======\n",
        "def encode_all_test_latents(model):\n",
        "    \"\"\"\n",
        "    Encode the entire test dataset into the latent space.\n",
        "\n",
        "    Parameters:\n",
        "    - model: VAE model (single or ensemble) used for encoding.\n",
        "\n",
        "    Returns:\n",
        "    - zs: Tensor of latent representations.\n",
        "    - ys: Corresponding labels.\n",
        "    \"\"\"\n",
        "    zs, ys = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in mnist_test_loader:\n",
        "            x = x.to(device)\n",
        "            q = model.encoder(x)\n",
        "            z = q.base_dist.loc\n",
        "            zs.append(z.cpu())\n",
        "            ys.append(y)\n",
        "    return torch.cat(zs, dim=0), torch.cat(ys, dim=0)\n",
        "\n",
        "# Encode full test set for each model\n",
        "latents_single = [encode_all_test_latents(model) for model in vae_single_list]\n",
        "latents_ensemble = [encode_all_test_latents(model) for model in vae_ensemble_list]"
      ],
      "metadata": {
        "id": "9nkLoXc1G_xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Plotting geodesics ======\n",
        "sample_steps = 256\n",
        "\n",
        "def plot_geodesics(latent_z, labels, z_pairs, curves, title, out_path):\n",
        "    \"\"\"\n",
        "    Plot geodesic paths in the latent space.\n",
        "\n",
        "    Parameters:\n",
        "    - latent_z: Latent representations of the test dataset.\n",
        "    - labels: Corresponding labels.\n",
        "    - z_pairs: Latent point pairs.\n",
        "    - curves: Geodesic curves (list of callables).\n",
        "    - title: Plot title.\n",
        "    - out_path: File path to save the plot.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    scatter = plt.scatter(latent_z[:, 0], latent_z[:, 1], c=labels, cmap=\"tab10\", s=8, alpha=0.4)\n",
        "    t_vals = torch.linspace(0, 1, sample_steps).to(device)\n",
        "\n",
        "    colors = ['r', 'g', 'b', 'm', 'c', 'y', 'orange', 'purple', 'gray', 'lime']\n",
        "\n",
        "    for i in range(len(curves)):\n",
        "        gamma = curves[i](t_vals).detach().cpu()\n",
        "        c0, c1 = z_pairs[i, 0], z_pairs[i, 1]\n",
        "        color = colors[i % len(colors)]\n",
        "        plt.plot(gamma[:, 0], gamma[:, 1], color=color, linewidth=1.5, label=f'Geodesic {i}')\n",
        "        plt.plot([c0[0], c1[0]], [c0[1], c1[1]], 'k--', linewidth=0.8)\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"z1\")\n",
        "    plt.ylabel(\"z2\")\n",
        "    plt.grid(True)\n",
        "    plt.legend(title=\"Class & Geodesics\", loc=\"best\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=300)\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "xRfejwLqfike"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Generate plots for all models ======\n",
        "for i in range(len(vae_single_list)):\n",
        "    z, y = latents_single[i]\n",
        "    plot_geodesics(\n",
        "        latent_z=z,\n",
        "        labels=y,\n",
        "        z_pairs=z_pairs_single_list[i].cpu(),\n",
        "        curves=all_curves_single[i],\n",
        "        title=f\"Geodesics in Latent Space (Single Decoder {i+1})\",\n",
        "        out_path=f\"vae_single_geodesics_{i+1}.png\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "oJMUMxrcyvq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(len(vae_ensemble_list)):\n",
        "    z, y = latents_ensemble[i]\n",
        "    plot_geodesics(\n",
        "        latent_z=z,\n",
        "        labels=y,\n",
        "        z_pairs=z_pairs_ensemble_list[i].cpu(),\n",
        "        curves=all_curves_ensemble[i],\n",
        "        title=f\"Geodesics in Latent Space (Ensemble Decoder {i+1})\",\n",
        "        out_path=f\"vae_ensemble_geodesics_{i+1}.png\"\n",
        "    )"
      ],
      "metadata": {
        "id": "cGADzRRByxnN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}